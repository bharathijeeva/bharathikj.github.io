---
layout: post
title:  "Sentence Redundancy Correction"
date:   2015-05-03
excerpt: "Identify and correct redundant phrases appearing in a Chinese sentence."
project: true
is_featured: true
feature_fig:
tag:
- Side Project
- Language model
- Natural language processing
- Computer Science
comments: true
---

<figure>
	<img src="{{site.url}}/assets/img/Redun/NLP.jpg">
</figure>

<center>
	<a href="https://github.com/momohuang/Sentence-Redundancy-Correction" target="_blank" class="btn">
		<span style="font-size: 120%;">
		GitHub
		</span>
	</a>
	&nbsp;
	<a href="{{ site.baseurl }}/assets/img/Redun/NLP_report.pdf" target="_blank" class="btn">
		<span style="font-size: 120%;">
		Tech. Report
		</span>
	</a>
</center>

<p>In this project, our goal is to identify and correct redundant words that appear in a Chinese sentence. This problem requires both identification and correction. For identification, we combine rule-based and language model based approaches. First, we used hypothesis test to find erroneous trigrams (consists of both POS and word) that collocate with redundancy. Then we extract probability information from the language model used in <a href="http://nlp.stanford.edu/software/tagger.shtml">Stanford POS tagger</a>. The two informations are combined through training a <a href="https://www.csie.ntu.edu.tw/~cjlin/liblinear/" target="_blank">logistic regression</a> model with features from both methods. For correction, we have tried two different strategy. One is to train a <a href="http://www.chokkan.org/software/crfsuite/" target="_blank">conditional random field</a> that can predict the label of each word segment (being redundant or not) in a sentence. Another attempt is to train a language model through accessing the raw data of <a href="http://storage.googleapis.com/books/ngrams/books/datasetsv2.html" target="_blank">Google N-gram</a>. Then we remove the word segment that can substantially boost the sentence's language model probability. Note that working with Google N-gram takes great effort, since the raw data of Google N-gram already takes several terabytes to store.</p>
